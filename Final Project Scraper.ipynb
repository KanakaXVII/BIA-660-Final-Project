{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bf7f4e",
   "metadata": {},
   "source": [
    "# BIA-660: Web Scraping\n",
    "**Final Project Part 1: Scraper**\n",
    "\n",
    "**Date:** \n",
    "\n",
    "**Team**\n",
    "- Jarrin Sacayanan\n",
    "- Sabah Ahmed\n",
    "- Million Mehari\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Steps\n",
    "1. Collect at least 5,000 Job Ads for Data Scientists from Indeed.com\n",
    "2. Collect at least 5,000 Job Ads for Software Engineers from Indeed.com\n",
    "3. Get the HTML of the job description (as shown on the right side of the screen after you click on an Ad) for each Ad.\n",
    "4. Extract the text from the HTML and create a CSV with 1 Ad per line and 2 columns: `<text>` and `<job title>`\n",
    "5. Train a classificatino model that can predict whether a given Ad is for a Data Scientist or Software Engineer\n",
    "\n",
    "### Notes\n",
    "- Your trained model will be evaluated on a separate test set that you will not have access to before the deadline\n",
    "- The deliverables include:\n",
    "    - The scraping script(s) in .ipynb format\n",
    "    - The classification script as a separate .ipynb Notebook\n",
    "    - Instructions on how to run the 2 Notebooks\n",
    "    - The CSV from step 4\n",
    "- Your classification script should be able t oread a test CSV that will include 1 job description per line (no labels). It should then produce a new file that includes the predicted label for each line in the test file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2412b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "889e81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, re, time,os\n",
    "import warnings\n",
    "\n",
    "# Scraping imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3764e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a warning filter for the driver deprication warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e7e7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some options for the webdriver\n",
    "chrome_options = Options() \n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "chrome_options.add_argument('--headless')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc79669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [C:\\Users\\jarri\\.wdm\\drivers\\chromedriver\\win32\\101.0.4951.41\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Create the Chrome webdriver to prepare for scraping\n",
    "service = Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97acbd",
   "metadata": {},
   "source": [
    "# Build a Scraping Function\n",
    "This function should perform scraping based on a passed URL and a scrape count and should result in a Pandas dataframe containing the desired contents. In this case, that is a dataframe with two columns:\n",
    "- `Job Title`\n",
    "- `Job Description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1ff1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed(driver, label, city, lim, scrape_log):\n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Init storage for scraping results\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    # Init storage for job IDs\n",
    "    seen_ids = []\n",
    "    \n",
    "    # Init a timeout counter\n",
    "    timeout = 0\n",
    "    \n",
    "    # Init counters to test for infinite page looping\n",
    "    page_loop_timeout = 0\n",
    "    previous_job_count = 0\n",
    "    \n",
    "    # Init a bool to break the infinite loop later\n",
    "    cont = True\n",
    "    \n",
    "    while timeout <= 3:\n",
    "        try:\n",
    "            # Go to the Indeed home page\n",
    "            url = 'https://www.indeed.com/'\n",
    "            driver.get(url)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Fill in the search fields\n",
    "            job_field = driver.find_element_by_css_selector('[id=\"text-input-what\"]')\n",
    "            job_field.send_keys(label)\n",
    "\n",
    "            where_field = driver.find_element_by_css_selector('[id=\"text-input-where\"]')\n",
    "            where_field.send_keys(Keys.CONTROL, 'a', Keys.DELETE)\n",
    "            where_field.send_keys(city)\n",
    "\n",
    "            # Perform the search\n",
    "            print('Searching now...')\n",
    "            scrape_log.write('Searching now...\\n')\n",
    "            time.sleep(2)\n",
    "            where_field.send_keys(Keys.ENTER)\n",
    "\n",
    "            # Wait before proceeding with page scrape\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Search Error...{e}')\n",
    "            scrape_log.write(f'Search Error...{e}\\n')\n",
    "\n",
    "        try:\n",
    "            # Collect from indefinite pages\n",
    "            while cont:\n",
    "                # Collect all jobs on screen\n",
    "                jobs = driver.find_elements_by_css_selector('[class=\"jcs-JobTitle\"]')\n",
    "                    \n",
    "                if len(jobs) == 0:\n",
    "                    print('No job listings collected...')\n",
    "                    scrape_log.write('No job listings collected...\\n')\n",
    "                else:\n",
    "                    print(f'Number of jobs on page: {len(jobs)}')\n",
    "                    scrape_log.write(f'Number of jobs on page: {len(jobs)}\\n')\n",
    "\n",
    "                # Loop through the job cards\n",
    "                for job in jobs:\n",
    "                    if len(new_df) < lim:\n",
    "                        try:\n",
    "                            # Make sure the job hasn't been collected already\n",
    "                            if job.get_attribute('id') not in seen_ids:\n",
    "                                seen_ids.append(job.get_attribute('id'))\n",
    "                            else:\n",
    "                                print(f'Job {job.get_attribute(\"id\")} already seen...')\n",
    "                                break\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f'Something went wrong...{e}')\n",
    "                            scrape_log.write(f'Something went wrong...{e}\\n')\n",
    "                          \n",
    "                        # Debug log\n",
    "                        print(f'Getting Job: {len(new_df) + 1}')\n",
    "                        scrape_log.write(f'Getting Job: {len(new_df) + 1}\\n')\n",
    "\n",
    "                        # Make it wait a second to prevent bot blocks\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        # Init a payload for the df\n",
    "                        payload = {}\n",
    "\n",
    "                        # Click on the job to open the job details on the right of the page\n",
    "                        try:\n",
    "                            job.click()\n",
    "                            time.sleep(1)\n",
    "                        except Exception as e:\n",
    "                            print(f'Cant click...{job.text}')\n",
    "                            scrape_log.write(f'Cant click...{job.text}\\n')\n",
    "                            break\n",
    "\n",
    "                        # Switch to the iframe context to collect the description\n",
    "                        description_frame = driver.find_element_by_css_selector('[id=\"vjs-container-iframe\"]')\n",
    "                        driver.switch_to.frame(description_frame)\n",
    "\n",
    "                        # Try and store the job title \n",
    "                        try:\n",
    "                            job_title = driver.find_element_by_css_selector('[class*=\"jobsearch-JobInfoHeader-title\"]')\n",
    "                            payload['Title'] = job_title.text.removesuffix('\\n- job post')\n",
    "                        except:\n",
    "                            payload['Title'] = None\n",
    "\n",
    "                        # Try and store the description\n",
    "                        try:\n",
    "                            job_description = driver.find_element_by_css_selector('[id=\"jobDescriptionText\"]')\n",
    "                            payload['Description'] = job_description.text\n",
    "                        except:\n",
    "                            payload['Description'] = None\n",
    "\n",
    "                        # Store the label\n",
    "                        payload['Label'] = label\n",
    "\n",
    "                        # Append the payload to the df\n",
    "                        new_df = new_df.append(payload, ignore_index=True)\n",
    "\n",
    "                        # Switch the context back to the list of jobs\n",
    "                        driver.switch_to.parent_frame()\n",
    "                    else:\n",
    "                        # Calculate end time\n",
    "                        end_time = time.time()\n",
    "                        delta = end_time - start_time\n",
    "\n",
    "                        # Close the webpage\n",
    "                        driver.close()\n",
    "\n",
    "                        # Return the dataframe if limit wasn't reached I guess\n",
    "                        return new_df, delta\n",
    "\n",
    "                # Scroll to the bottom of the page where the next button is at\n",
    "                driver.execute_script('window,scrollTo(0,document.body.scrollHeight)')\n",
    "\n",
    "                # Look for the next page button\n",
    "                try:\n",
    "                    # Find the next button\n",
    "                    next_button = WebDriverWait(driver,15).until(EC.presence_of_element_located((By.CSS_SELECTOR,'[aria-label=\"Next\"]')))\n",
    "                    print('Next button found')\n",
    "                    scrape_log.write('Next button found\\n')\n",
    "                    \n",
    "                    # Get the current length of the results\n",
    "                    if len(new_df) == previous_job_count and page_loop_timeout < 3:\n",
    "                        # Add to the page loop counter\n",
    "                        page_loop_timeout += 1\n",
    "                    \n",
    "                    elif len(new_df) == previous_job_count and page_loop_timeout >= 3:\n",
    "                        # Break the loop because it is stuck\n",
    "                        print('Pages looping without grabbing jobs. Breaking loop.')\n",
    "                        scrape_log.write('Pages looping without grabbing jobs. Breaking loop.\\n')\n",
    "                        \n",
    "                        # Calculate end time\n",
    "                        end_time = time.time()\n",
    "                        delta = end_time - start_time\n",
    "\n",
    "                        # Close the webpage\n",
    "                        driver.close()\n",
    "\n",
    "                        # Return the dataframe if limit wasn't reached I guess\n",
    "                        return new_df, delta\n",
    "                    \n",
    "                    # Click the button\n",
    "                    next_button.click()\n",
    "                    time.sleep(3)\n",
    "                        \n",
    "                except:\n",
    "                    next_button = None\n",
    "                    print('Next button not found')\n",
    "                    scrape_log.write('Next button not found\\n')\n",
    "\n",
    "                # Check for the final page\n",
    "                if next_button is None and timeout >= 2:\n",
    "                    print('Last page reached.')\n",
    "                    scrape_log.write('Last page reached.\\n')\n",
    "                    \n",
    "                    # Calculate end time\n",
    "                    end_time = time.time()\n",
    "                    delta = end_time - start_time\n",
    "\n",
    "                    # Close the webpage\n",
    "                    driver.close()\n",
    "\n",
    "                    # Return the dataframe if limit wasn't reached I guess\n",
    "                    return new_df, delta\n",
    "                \n",
    "                elif next_button is None and timeout < 2:\n",
    "                    print(f'Timeout: {timeout} - Retrying...')\n",
    "                    timeout += 1\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Job scrape error...')\n",
    "            scrape_log.write('Job Scrape error...\\n')\n",
    "            if timeout < 3:\n",
    "                print(f'Timeout: {timeout}')\n",
    "                scrape_log.write(f'Timeout: {timeout}\\n')\n",
    "                timeout += 1\n",
    "            else:\n",
    "                print('Timed out...moving on...')\n",
    "                scrape_log.write('Timed out...moving on...\\n')\n",
    "                \n",
    "                # Calculate end time\n",
    "                end_time = time.time()\n",
    "                delta = end_time - start_time\n",
    "\n",
    "                # Close the webpage\n",
    "                driver.close()\n",
    "\n",
    "                # Return the dataframe if limit wasn't reached I guess\n",
    "                return new_df, delta\n",
    "            \n",
    "    # Calculate end time\n",
    "    end_time = time.time()\n",
    "    delta = end_time - start_time\n",
    "    \n",
    "    # Close the webpage\n",
    "    driver.close()\n",
    "    \n",
    "    # Return the dataframe if limit wasn't reached I guess\n",
    "    return new_df, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f9303cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function to perform a search and log it\n",
    "def setup_scrape(job_title, scrape_count):\n",
    "    # Init storage\n",
    "    scrape_df = pd.DataFrame()\n",
    "\n",
    "    # Get list of 100 biggest cities in US\n",
    "    cities = pd.read_csv('cities.txt', header=None)\n",
    "    cities.columns = ['City', 'State']\n",
    "    city_list = [f'{city},{state}' for city, state in zip(cities['City'], cities['State'])]\n",
    "\n",
    "    # Get the current time for the log filename\n",
    "    current_time = time.localtime()\n",
    "    current_time = time.strftime('%H-%M-%S', current_time)\n",
    "\n",
    "    # Build a file name\n",
    "    log_file_name = f'{current_time}_{job_title}_scrape_log.txt'\n",
    "\n",
    "    # Create a scraper log to track what's going on\n",
    "    with open(log_file_name, 'w') as scrape_log:\n",
    "        # Iterate through the cities and run the scraper for each one\n",
    "        for city in city_list:\n",
    "            # Add a logging line\n",
    "            print(f'\\nCurrent Result Count: {len(scrape_df)}\\nStarting scrape {city}')\n",
    "            scrape_log.write(f'\\nCurrent Result Count: {len(scrape_df)}\\nStarting scrape {city}\\n')\n",
    "\n",
    "            # Create the Chrome webdriver to prepare for scraping\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Call the function\n",
    "            results, timer = scrape_indeed(driver, job_title, city, scrape_count, scrape_log)\n",
    "\n",
    "            # Store the results\n",
    "            scrape_df = scrape_df.append(results, ignore_index=True)\n",
    "\n",
    "    # Close the scrape log\n",
    "    scrape_log.close()\n",
    "    print(f'Log File: {log_file_name}')\n",
    "    \n",
    "    # Return the results\n",
    "    return scrape_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f5ac4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to name the output file\n",
    "def get_file_name(job_acr):\n",
    "    if job_acr in ['ds', 'se']:\n",
    "        new_file = ''\n",
    "        while new_file == '':\n",
    "            try:\n",
    "                current_files = os.listdir('scrapes/')\n",
    "\n",
    "                ds_scrapes = []\n",
    "                for file in current_files:\n",
    "                    if job_acr in file:\n",
    "                        ds_scrapes.append(file)\n",
    "\n",
    "                file_inds = []\n",
    "                for file in ds_scrapes:\n",
    "                    file = file.rstrip('.csv')\n",
    "                    file = file.split('_')\n",
    "                    file_inds.append(file[2])\n",
    "\n",
    "                new_file = f'scrapes/{job_acr}_scrape_{int(file_inds[-1]) + 1}.csv'\n",
    "                \n",
    "                return new_file\n",
    "            except FileNotFoundError:\n",
    "                print(f'Directory not found. Creating new directory.')\n",
    "                try:\n",
    "                    os.mkdir('test/')\n",
    "                except Exception as e:\n",
    "                    print('Something went wrong...{e}')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        print('File name not defined. Use either \"ds\" or \"se\".')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a0485",
   "metadata": {},
   "source": [
    "# Performing Scrapes\n",
    "**Instructions**\n",
    "\n",
    "The function must be called using the following format:\n",
    "\n",
    "`<result dataframe> = setup_scrape(<job_title>, <job listings per city>)`\n",
    "\n",
    "The scraper is designed to perform 100 different scrapes: one for each of the 100 most populated cities in the United States. The result is a single dataframe with all of the results appended. The `label` column will be added to the dataframe with a label matching the `<job title>` string parameter that is passed in the function.\n",
    "\n",
    "We will be running the scraper to collect dataframes for:\n",
    "- Data Scientist\n",
    "- Software Engineer\n",
    "\n",
    "The resulting dataframes will be exported as CSV files to use in the second script `Final Project Classification.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a192f",
   "metadata": {},
   "source": [
    "## Scrape Data Scientist Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e42360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Result Count: 0\n",
      "Starting scrape New York City, New York\n",
      "Searching now...\n",
      "No job listings collected...\n",
      "Next button found\n",
      "No job listings collected...\n",
      "Next button found\n",
      "Next button not found\n",
      "Timeout: 0 - Retrying...\n",
      "Searching now...\n",
      "No job listings collected...\n",
      "Next button found\n",
      "No job listings collected...\n",
      "Next button found\n",
      "Pages looping without grabbing jobs. Breaking loop.\n",
      "\n",
      "Current Result Count: 0\n",
      "Starting scrape Los Angeles, California\n",
      "Searching now...\n",
      "No job listings collected...\n",
      "Next button found\n",
      "No job listings collected...\n",
      "Next button found\n",
      "Next button not found\n",
      "Timeout: 0 - Retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Collect Data Scientist jobs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds_df \u001b[38;5;241m=\u001b[39m \u001b[43msetup_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData Scientist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36msetup_scrape\u001b[1;34m(job_title, scrape_count)\u001b[0m\n\u001b[0;32m     29\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m results, timer \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_indeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Store the results\u001b[39;00m\n\u001b[0;32m     35\u001b[0m scrape_df \u001b[38;5;241m=\u001b[39m scrape_df\u001b[38;5;241m.\u001b[39mappend(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36mscrape_indeed\u001b[1;34m(driver, label, city, lim, scrape_log)\u001b[0m\n\u001b[0;32m     24\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.indeed.com/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Fill in the search fields\u001b[39;00m\n\u001b[0;32m     30\u001b[0m job_field \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_css_selector(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-input-what\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collect Data Scientist jobs\n",
    "ds_df = setup_scrape('Data Scientist', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf838f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = pd.DataFrame()\n",
    "ds_df['Test'] = [1, 2, 3]\n",
    "ds_df['Test 2'] = [4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.to_csv(get_file_name('ds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee022fd3",
   "metadata": {},
   "source": [
    "## Scrape Software Engineer Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6949c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect software engineer jobs\n",
    "se_df = setup_scrape('Software Engineer', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb59a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_df.to_csv(get_file_name('se'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
